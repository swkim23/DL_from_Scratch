\documentclass[12pt]{article}

\include{defs}

%%%%%%%%%%%%%%%
% Title Page
\title{ccOS Lifecycle}
\author{Infotainment Software Development Team \newline Hyundai Motor Company \newline }
\date{\today}
%%%%%%%%%%%%%%%

\begin{document}
	\maketitle
	
	\tableofcontents
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	% 본문의 시작 
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	\section{서론}
	
	편향(bias)는 뉴런이 얼마나 쉽게 활성화 되는지를 제어한다.
	가중치는 각 신호의 영향력을 의미한다.
	
	\subsection{목적}
	
	\begin{figure}[!h] %신경망 예시 그림
		\centering
		\includegraphics[width=0.8\textwidth]{fig/fig-3-1.png}
		\caption{신경망의 예}
	\end{figure}
	
	
	Perceptron은 복잡한 함수도 표현이 가능하다. 심지어 다층 퍼셉트론은 컴퓨터도 표현 가능하다.
	하지만, 퍼셉트론의 가중치를 설정하는 것은 여전히 사람에 의해서만 가능하다.
	
	신경망은 데이터로 부터 퍼셉트론의 가중치 설정을 자동으로 한다.
	
	\clearpage
	\section{신경망}
	입력층, 은닉층, 출력층 으로 구성
	활성화 함수 - 입력 신호의 총합을 출력 신호로 변환
	
	활성화 함수를 계단 함수에서 다른 함수로 변경하는 것이 신경망의 세계로 나아가는 열쇠이다.
	
	% $f(x)=\frac{1}{1+e^{-x}}$
	\[ f(x)=\frac{1}{1+\exp^(-x)}	\]
	
	신경망의 활성화 함수는 비선형 함수를 사용해야 한다. 여기서 비선형이란 하나의 직선으로 표현할 수 없는 함수를 의미한다.
	비선형 함수를 사용하는 이유는 선형 함수를 사용할 경우 신경망 층을 깊게 하면 의미가 사라지기 때문이다.
	
	신경망은 분류와 회귀에 이용할 수 있다. 이는 출력층의 활성화 함수의 선택을 결정 짓는 요인이 된다. 회귀에는 항등함수를 사용하고 분류에는 소프트맥스함수를 사용한다.
	
	\subsection{소프트맥스 함수의 특징}
	소프트맥스 함수의 출력은 0과 1.0사이의 실수이며, 함수 출력 총합은 1이다.
	이러한 성질을 이용하면 소프트맥스 함수의 출력을 확률로 생각할 수 있다.
	\[ f(v) = \frac{\exp(a_{k})} {\sum_{i=1}^{n}  {\exp(a_{i})}} \]
	
	소프트맥스 함수는 단조 증가 함수이기 때문에 소프트맥스 함수를 활성함수로 적용해도 각 원소의 대소 관계는 변화가 없다. 따라서, 신경망을 이용하여 분류를 할때 출력층의 소프트맥스 함수를 생략할 수 있다.
	
	\subsection{궁금한점}
	하지만, 분류에 소프트맥스 함수를 생략 할 수 있다면 결국 항등 함수와 무엇이 다른가?
	\clearpage
	
\section{신경망 학습}
학습이한 데이터로 부터 가중치 매개변수의 최적값을 자동으로 획득한다.
문제는 매개 변수가 최적값임을 판단하기 위한 기준이 필요 하다.
신경망이 학습을 위한 지표는 손실 함수를 사용한다.

딥러닝은 종단간 기계학습(end-to-end machine learning)이라고도 한다. 데이터(입력)에서 목표한 결과(출력)을 사람의 개입없이 얻는다는 의미.

\subsection{손실함수}

손실함수는 평균 제곱 오차(mean squared error, MSE)와 교차 엔트로피 오차(cross entropy error, CEE) 둘이 자주 사용된다.
\subsubsection{평균 제곱 오차}
평균 제곱 오차의 식은 다음과 같다.
\[ E = \frac{1}{2} \sum_{k}(y_{k} - t_{k})^2  \]

이때 궁금한 점은 평균임에도 불구하고 왜 n이 아닌 2로 나누는 것인가? 이는 y값과 t값이 갖는 범위를 생각해 보면 이해할 수 있다. 먼저 y는 소프트맥스함수 값이므로 $y_{k}$ 총 합은 항상 1이다.
$t_{k}$는 원-핫 인코딩으로 표기 했기 때문에 정답에 해당하는 값만 1이고 나머지는 0이다. 따라서 $t_{k}$의 총합 또한 항상 1이다. 따라서 이들의 평균을 구하기 위해서는 N이 아닌 2로 나누어야 한다.

평균 제곱 오차의 값이 작을 수록 정답에 가깝고 값이 클 수록 정답이 아님을 의미한다.
\subsubsection{교차 엔트로피}
교차 엔트로피는 

\end{document}          
